1. Why is big data important?
2. What do the 3Vs of big data mean?
3. What is the ETL cycle?
4. What is the difference between stream and batch processing?
___
1. 2.5 Exabytes produced daily and only expected to rise, efficient and fast ways of handeling this data are needed
2. the main 3Vs are
	1. **Volume**: large amounts of data
	2. **Variety**: data comes in many different forms from diverse sources
	3. **Velocity**: the content is changing quickly
- the more Vs are
	1. **Value**: data alone is not enough; how can value be derived from it?
	2. **Validity**: ensure that the interpreted data is sound, clean
	3. **Veracity**: can we trust the data? Does it come from reliable sources?
	4. **Volatility**: how long does data need to be kept for? When is it considered irrelevant?
	5. **Visibility**: data from diverse sources need to be stitched together
	6. **Virality**: how quickly data is spread and shared
3. The **ETL** cycle
	- **E**xtract: Convert raw or semi-structured data into structured data
	- **T**ransform: Convert units, join data sources, cleanup etc
	- **L**oad: Load the data into another system for further processing
		- Big data _engineering_ is concerned with building _pipelines_
		- Big data _analytics_ is concerned with _discovering patterns_
4. The difference between stream and batch processing is: 
	- **Batch Processing**: all data exists in some data store,  a program processes the whole dataset at once
	- **Stream processing**: Processing of data as they arrive to the system
- 2 basic approaches to distribute data processing operations on lots of machines
	- Divide the data in chunks, apply the same algorithm on all chunks(**Data-parallelism**)
	- Divide the problem in chunks, run it on a cluster of machines(**Task-parallelism**)
#### Volume
We call Big Data big because it is _really big_:
- 90% of all the data ever [was created in the last 2 years](http://www.sintef.no/en/latest-news/big-data-for-better-or-worse/)
- Every human created [at least 1.7 MB of data per second](https://techjury.net/blog/how-much-data-is-created-every-day) in 2020.
- Every day, 306.4 billion emails are sent, and 500 million Tweets are made.
- The global big data and business analytics market was valued at [$138.9 billion in 2020 and is expected to grow to $229 billion in 2025]([https://www.precisely.com/blog/big-data/quality-data-big-data-worth](https://www.precisely.com/blog/big-data/quality-data-big-data-worth)).
#### Variety
- Structured data: SQL tables, format is known
- Semi-structured data: JSON, XML
- Unstructured data: Text, audio, video
We often need to combine various data sources of different types to come up with a result
#### Velocity
Data is not just big; it is generated and needs to be processed fast. Think of:
- Data centers writing to log files
- IoT sensors reporting temperatures around the globe
- Twitter: >500 million tweets a day (or 6k/sec)
- Stock Markets: high-frequency trading (latency costs money)
- Online advertising
Data needs to be processed with soft or hard real-time guarantees