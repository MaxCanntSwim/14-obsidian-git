1. What are Spark RDDs? Why was Spark so revolutionary?
2. What is the difference between RDDs and Pair RDDs? Why do we need both?
3. What are the key Spark API calls?
4. What are Spark transformations or actions?
5. What are wide and narrow dependencies?
6. How does Spark deal with faults?
7. What types of partitioning can we employ for dist systems like Spark?
8. How does Catalyst optimize queries?
___
1. Spark RDDs are work like List\[A] in from scala. RDDs make datasets distributed over a cluster of machines look like a Scala collection. RDDs:
	- are immutable
	- reside (mostly) in memory
	- are transparently distributed
	- feature all FP primitives
		- in addition, more to minimise shuffling




# Spark RDDs
![[Pasted image 20231105214505.png]]
## Pair RDDs
RDDs can contain any data type, if it can be iterated. RDDs of type `RDD[(k,v)]` are special in spark, they can be both iterated and indexed. 
Operations such as `join` are only defined on Pair RDDs, meaning that we can only combine RDDs if their contents can be indexed.
We can create Pair RDDs by applying an indexing function, using `keyBy` function, or by grouping records:
```Scala
val rdd = sc.parallelize(List("foo", "bar", "baz")) // RDD[String]

val pairRDD = rdd.map(x => (x.charAt(0), x))  // RDD[(Char, String)]
pairRDD.collect // Array((f,foo), (b,bar), (b,baz))

val pairRDD2 = rdd.keyBy(x => x.toLowerCase.head) // RDD[(Char, String)]
pairRDD2.collect // Array((f,foo), (b,bar), (b,baz))

val pairRDD3 = rdd.groupBy(x => x.charAt(0))  // RDD[(Char, Iterable(String))]
pairRDD3.collect // Array((b,CompactBuffer(bar, baz)), (f,CompactBuffer(foo)))
```
